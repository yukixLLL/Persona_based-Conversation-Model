{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from encoder import * \n",
    "from decoder import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor_test = np.load(PATH + 'd_tensor_test.npy',allow_pickle=True)\n",
    "r_tensor_test = np.load(PATH + 'r_tensor_test.npy',allow_pickle=True)\n",
    "dia_test = [t[0] for t in d_tensor_test]\n",
    "aid_test = [t[1] for t in d_tensor_test]\n",
    "res_test = [t[0] for t in r_tensor_test]\n",
    "sid_test = [t[1] for t in r_tensor_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH + 'tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 96\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYER = 4\n",
    "DROP_OUT = 0.2\n",
    "embedding_dim = 512\n",
    "speaker_dim = 128\n",
    "MAXLEN = 50\n",
    "speakerNum = 14\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response with beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '../persona_ckpt_512_sp128/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.training.tracking.util.CheckpointLoadStatus object at 0xb389947b8>\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(HIDDEN_SIZE, vocab_size, embedding_dim, NUM_LAYER, BATCH_SIZE)\n",
    "decoder = Decoder(HIDDEN_SIZE, vocab_size, embedding_dim, speaker_dim,NUM_LAYER)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "cp = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                         encoder=encoder,\n",
    "                         decoder=decoder)\n",
    "status = cp.restore(checkpoint_dir + \"speaker-add-ckpt-4\")\n",
    "print(status)\n",
    "train_nn = Train(encoder, decoder, optimizer, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_word(response_sentence,tokenizer):\n",
    "    sentence = list()\n",
    "    for idx in response_sentence:\n",
    "        sentence.append(tokenizer.index_word[idx])\n",
    "        if(tokenizer.index_word[idx]=='<eos>'):\n",
    "            break;\n",
    "    print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<sos> ' + w + ' <eos>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conver_to_tensor(sentence,tokenizer):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = [tokenizer.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                         maxlen=MAXLEN,\n",
    "                                                         padding='post')\n",
    "    tensor = tf.convert_to_tensor(inputs,dtype=np.int32)\n",
    "    return tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(train_nn,inp,speaker_id,addressee_id=None,max_len = MAXLEN,beam_size = 2):\n",
    "    generate_size = beam_size\n",
    "    inputs = np.expand_dims(inp, axis=0)\n",
    "    s_id = np.expand_dims(speaker_id, axis=0)\n",
    "    if addressee_id is not None:\n",
    "        a_id = np.expand_dims(addressee_id, axis=0)\n",
    "    enc_hidden = [tf.zeros((1, HIDDEN_SIZE)),tf.zeros((1, HIDDEN_SIZE))]\n",
    "    enc_out, enc_hidden,enc_c = train_nn.encoder(inputs, enc_hidden)\n",
    "    dec_init_state = [enc_hidden,enc_c]\n",
    "    dec_input = tf.convert_to_tensor([train_nn.tokenizer.word_index['<sos>']])\n",
    "\n",
    "    sos = train_nn.tokenizer.word_index['<sos>']\n",
    "    dec_input = tf.expand_dims(dec_input,0)\n",
    "\n",
    "    if addressee_id is not None:\n",
    "        predictions, dec_hidden, dec_c,_ = train_nn.decoder(dec_input,enc_out, dec_init_state,s_id,a_id)\n",
    "    else:\n",
    "        predictions, dec_hidden, dec_c,_ = train_nn.decoder(dec_input,enc_out, dec_init_state,s_id)\n",
    "\n",
    "    dec_init_state = [dec_hidden,dec_c]\n",
    "    pred_prob,pred_top_k = tf.math.top_k(predictions,k=beam_size)\n",
    "#     for i in range(pred_prob[0].shape[0]):\n",
    "#                 print(\"prob:{},word:{}\".format(pred_prob[0][i],tokenizer.index_word[pred_top_k[0][i].numpy()]))\n",
    "    # beam: (prob,sentence,dec_inp,dec_init,isEnd)\n",
    "    finished = list() # to store finished sentence\n",
    "    beam = list()\n",
    "    for ind, i in enumerate(pred_top_k[0]):\n",
    "        word_idx = i.numpy()\n",
    "        if tokenizer.index_word[word_idx]=='<eos>':\n",
    "            finished.append([pred_prob[0][ind],[sos]+[word_idx],i,dec_init_state,True])\n",
    "        beam.append([pred_prob[0][ind],[sos]+[i.numpy()],i,dec_init_state,False])\n",
    "    for t in range(2,max_len):\n",
    "        tmp_beam = list()\n",
    "        for ind,b in enumerate(beam):\n",
    "            if b[-1]:\n",
    "                continue\n",
    "            dec_input = tf.expand_dims(tf.convert_to_tensor([b[2]]),0)\n",
    "            dec_init_state = b[3]\n",
    "            if addressee_id is not None:\n",
    "                predictions, dec_hidden, dec_c,_ = train_nn.decoder(dec_input,enc_out, dec_init_state,s_id,a_id)\n",
    "            else:\n",
    "                predictions, dec_hidden, dec_c,_ = train_nn.decoder(dec_input,enc_out, dec_init_state,s_id)\n",
    "            \n",
    "            dec_init_state = [dec_hidden,dec_c]\n",
    "\n",
    "            # use beam search to get top k prediction\n",
    "            pred_prob,pred_top_k = tf.math.top_k(predictions,k=beam_size)\n",
    "#             for i in range(pred_prob[0].shape[0]):\n",
    "#                 print(\"beam:{} prob:{},word:{}\".format(ind,pred_prob[0][i],tokenizer.index_word[pred_top_k[0][i].numpy()]))\n",
    "            beam_prob = beam[ind][0]\n",
    "            beam_sen = beam[ind][1]\n",
    "            for ind, i in enumerate(pred_top_k[0]):\n",
    "                isEnd = False\n",
    "                word_idx = i.numpy()\n",
    "                if tokenizer.index_word[word_idx]=='<eos>':\n",
    "                    isEnd = True\n",
    "                tmp_beam.append([(beam_prob + pred_prob[0][ind])/(t+1),beam_sen + [word_idx],i,dec_init_state,isEnd])\n",
    "                \n",
    "        # select top k candidates combination\n",
    "        tmp_beam.sort(key=lambda x:x[0],reverse=True)\n",
    "        tmp_beam = tmp_beam[:generate_size]\n",
    "        beam = list()\n",
    "        for ind, b in enumerate(tmp_beam):\n",
    "            if b[-1]:\n",
    "                finished.append(b)\n",
    "                generate_size = generate_size - 1\n",
    "            else:\n",
    "                tmp_beam[ind][0] = b[0] * (t+1) # continue to compute probability of whole sentence\n",
    "                beam.append(tmp_beam[ind])\n",
    "        \n",
    "    finished.sort(key=lambda x:x[0],reverse=True)\n",
    "    print(\"{} Finished sentence:\".format(len(finished)))\n",
    "    for i in range(len(finished)):\n",
    "        convert_to_word(finished[i][1],train_nn.tokenizer)\n",
    "#     response_sentence = finished[:beam_size]\n",
    "#     print(\"Response sentence:\")\n",
    "#     for i in range(beam_size):\n",
    "#         convert_to_word(response_sentence[i][1],train_nn.tokenizer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2 = 1291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> i m being awful ? you re the one who went out of your way to hurt me . <eos>\n",
      "<sos> because you were being selfish . <eos>\n",
      "RESPONSE:\n",
      "3 Finished sentence:\n",
      "<sos> what ? <eos>\n",
      "<sos> i m sorry . <eos>\n",
      "<sos> i m sorry , i m sorry . <eos>\n",
      "addressee:7\n",
      "speaker:6\n"
     ]
    }
   ],
   "source": [
    "convert_to_word(dia_test[idx2],tokenizer)\n",
    "convert_to_word(res_test[idx2],tokenizer)\n",
    "print(\"RESPONSE:\")\n",
    "response(train_nn,dia_test[idx2],sid_test[idx2],aid_test[idx2],beam_size=3)\n",
    "print(\"addressee:{}\".format(aid_test[idx2]))\n",
    "print(\"speaker:{}\".format(sid_test[idx2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> i m being awful ? you re the one who went out of your way to hurt me . <eos>\n",
      "<sos> because you were being selfish . <eos>\n",
      "RESPONSE:\n",
      "3 Finished sentence:\n",
      "<sos> i m sorry . <eos>\n",
      "<sos> i m sorry , i m sorry . <eos>\n",
      "<sos> i m sorry , i m sorry . i m sorry . <eos>\n",
      "addressee:7\n",
      "speaker:6\n"
     ]
    }
   ],
   "source": [
    "addressee = tf.convert_to_tensor(8)\n",
    "convert_to_word(dia_test[idx2],tokenizer)\n",
    "convert_to_word(res_test[idx2],tokenizer)\n",
    "print(\"RESPONSE:\")\n",
    "response(train_nn,dia_test[idx2],sid_test[idx2],addressee,beam_size=3)\n",
    "print(\"addressee:{}\".format(aid_test[idx2]))\n",
    "print(\"speaker:{}\".format(sid_test[idx2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> i m being awful ? you re the one who went out of your way to hurt me . <eos>\n",
      "<sos> because you were being selfish . <eos>\n",
      "RESPONSE:\n",
      "3 Finished sentence:\n",
      "<sos> what ? <eos>\n",
      "<sos> i m sorry . <eos>\n",
      "<sos> i m sorry , i m sorry . <eos>\n"
     ]
    }
   ],
   "source": [
    "addressee = tf.convert_to_tensor(10)\n",
    "convert_to_word(dia_test[idx2],tokenizer)\n",
    "convert_to_word(res_test[idx2],tokenizer)\n",
    "print(\"RESPONSE:\")\n",
    "response(train_nn,dia_test[idx2],sid_test[idx2],addressee,beam_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> well , no , no , wait , wait , wait . all right , i gotta go . just listen . promise me , that you will wait a minute before you call her . <eos>\n",
      "<sos> ok . why ? <eos>\n",
      "RESPONSE:\n",
      "4 Finished sentence:\n",
      "<sos> hey ! <eos>\n",
      "<sos> oh , i m sorry . <eos>\n",
      "<sos> oh , i m sorry , i m sorry . <eos>\n",
      "<sos> <eos>\n",
      "addressee:5\n",
      "speaker:0\n"
     ]
    }
   ],
   "source": [
    "convert_to_word(dia_test[idx],tokenizer)\n",
    "convert_to_word(res_test[idx],tokenizer)\n",
    "print(\"RESPONSE:\")\n",
    "response(train_nn,dia_test[idx],sid_test[idx],aid_test[idx],beam_size=3)\n",
    "print(\"addressee:{}\".format(aid_test[idx]))\n",
    "print(\"speaker:{}\".format(sid_test[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> well , no , no , wait , wait , wait . all right , i gotta go . just listen . promise me , that you will wait a minute before you call her . <eos>\n",
      "<sos> ok . why ? <eos>\n",
      "RESPONSE:\n",
      "3 Finished sentence:\n",
      "<sos> what ? <eos>\n",
      "<sos> i m sorry . <eos>\n",
      "<sos> i m sorry , i m sorry . <eos>\n"
     ]
    }
   ],
   "source": [
    "addressee = tf.convert_to_tensor(2)\n",
    "convert_to_word(dia_test[idx],tokenizer)\n",
    "convert_to_word(res_test[idx],tokenizer)\n",
    "print(\"RESPONSE:\")\n",
    "response(train_nn,dia_test[idx],sid_test[idx2],addressee,beam_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> well , no , no , wait , wait , wait . all right , i gotta go . just listen . promise me , that you will wait a minute before you call her . <eos>\n",
      "<sos> ok . why ? <eos>\n",
      "RESPONSE:\n",
      "3 Finished sentence:\n",
      "<sos> what ? <eos>\n",
      "<sos> i m sorry . <eos>\n",
      "<sos> i m sorry , i m sorry . <eos>\n"
     ]
    }
   ],
   "source": [
    "addressee = tf.convert_to_tensor(3)\n",
    "convert_to_word(dia_test[idx],tokenizer)\n",
    "convert_to_word(res_test[idx],tokenizer)\n",
    "print(\"RESPONSE:\")\n",
    "response(train_nn,dia_test[idx],sid_test[idx2],addressee,beam_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
