{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "friends = 'friends.csv'\n",
    "bigbang = 'bigbang.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "friends_df = pd.read_csv(path+friends)\n",
    "bigbang_df = pd.read_csv(path+bigbang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episodes</th>\n",
       "      <th>speakers</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [episodes, speakers, dialogue]\n",
       "Index: []"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "friends_df[friends_df['dialogue'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([15625, 19508, 30537, 32482, 36957, 38247], dtype='int64')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigbang_df[bigbang_df['dialogue'].isna()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episodes</th>\n",
       "      <th>speakers</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>speaker_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>joey</td>\n",
       "      <td>I'm tellin' ya that girl totally winked at me.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>other</td>\n",
       "      <td>Did not, she did not wink at you....</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>chandler</td>\n",
       "      <td>Huh.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>ross</td>\n",
       "      <td>I have to say Tupolo Honey by Van Morrison.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>rachel</td>\n",
       "      <td>Nooo Way! The most romantic song ever is The ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episodes  speakers                                           dialogue  \\\n",
       "0        31      joey     I'm tellin' ya that girl totally winked at me.   \n",
       "1        31     other               Did not, she did not wink at you....   \n",
       "2        31  chandler                                              Huh.    \n",
       "3        31      ross        I have to say Tupolo Honey by Van Morrison.   \n",
       "4        31    rachel   Nooo Way! The most romantic song ever is The ...   \n",
       "\n",
       "   speaker_id  \n",
       "0           1  \n",
       "1          14  \n",
       "2           3  \n",
       "3           5  \n",
       "4           2  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112706"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(friends_df)+len(bigbang_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112706"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([friends_df, bigbang_df], ignore_index=True, sort=False)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episodes</th>\n",
       "      <th>speakers</th>\n",
       "      <th>dialogue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>joey</td>\n",
       "      <td>I'm tellin' ya that girl totally winked at me.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>other</td>\n",
       "      <td>Did not, she did not wink at you....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>chandler</td>\n",
       "      <td>Huh.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>ross</td>\n",
       "      <td>I have to say Tupolo Honey by Van Morrison.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>rachel</td>\n",
       "      <td>Nooo Way! The most romantic song ever is The ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episodes  speakers                                           dialogue\n",
       "0        31      joey     I'm tellin' ya that girl totally winked at me.\n",
       "1        31     other               Did not, she did not wink at you....\n",
       "2        31  chandler                                              Huh. \n",
       "3        31      ross        I have to say Tupolo Honey by Van Morrison.\n",
       "4        31    rachel   Nooo Way! The most romantic song ever is The ..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_c = ['joey','rachel','chandler','monica','ross','phoebe','leonard','sheldon','penny','howard','raj','amy','bernadette','other']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amy': 12,\n",
       " 'bernadette': 13,\n",
       " 'chandler': 3,\n",
       " 'howard': 10,\n",
       " 'joey': 1,\n",
       " 'leonard': 7,\n",
       " 'monica': 4,\n",
       " 'other': 14,\n",
       " 'penny': 9,\n",
       " 'phoebe': 6,\n",
       " 'rachel': 2,\n",
       " 'raj': 11,\n",
       " 'ross': 5,\n",
       " 'sheldon': 8}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers_ind=dict()\n",
    "for ind, c in enumerate(main_c,1):\n",
    "    speakers_ind[c] = ind\n",
    "speakers_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episodes</th>\n",
       "      <th>speakers</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>speaker_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31</td>\n",
       "      <td>joey</td>\n",
       "      <td>I'm tellin' ya that girl totally winked at me.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>other</td>\n",
       "      <td>Did not, she did not wink at you....</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>chandler</td>\n",
       "      <td>Huh.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>ross</td>\n",
       "      <td>I have to say Tupolo Honey by Van Morrison.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31</td>\n",
       "      <td>rachel</td>\n",
       "      <td>Nooo Way! The most romantic song ever is The ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   episodes  speakers                                           dialogue  \\\n",
       "0        31      joey     I'm tellin' ya that girl totally winked at me.   \n",
       "1        31     other               Did not, she did not wink at you....   \n",
       "2        31  chandler                                              Huh.    \n",
       "3        31      ross        I have to say Tupolo Honey by Van Morrison.   \n",
       "4        31    rachel   Nooo Way! The most romantic song ever is The ...   \n",
       "\n",
       "   speaker_id  \n",
       "0           1  \n",
       "1          14  \n",
       "2           3  \n",
       "3           5  \n",
       "4           2  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['speaker_id'] = df['speakers'].apply(lambda x: speakers_ind[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 3]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakerid_list = list(df['speaker_id'])\n",
    "speakerid_list[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = list(df['speakers'])\n",
    "dialogues = list(df['dialogue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = list(df['episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    if isinstance(w,float):\n",
    "        w = '<SOS> <EOS>'\n",
    "        return w\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<SOS> ' + w + ' <EOS>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 12\n",
    "isinstance(x, int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> i m tellin ya that girl totally winked at me . <EOS>\n",
      "b'<SOS> i m tellin ya that girl totally winked at me . <EOS>'\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(dialogues[0]))\n",
    "print(preprocess_sentence(dialogues[0]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episodes</th>\n",
       "      <th>speakers</th>\n",
       "      <th>dialogue</th>\n",
       "      <th>speaker_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77158</th>\n",
       "      <td>102</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81041</th>\n",
       "      <td>1019</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92070</th>\n",
       "      <td>219</td>\n",
       "      <td>penny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94015</th>\n",
       "      <td>35</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98490</th>\n",
       "      <td>12</td>\n",
       "      <td>sheldon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99780</th>\n",
       "      <td>17</td>\n",
       "      <td>other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       episodes speakers dialogue  speaker_id\n",
       "77158       102  sheldon      NaN           8\n",
       "81041      1019    other      NaN          14\n",
       "92070       219    penny      NaN           9\n",
       "94015        35  sheldon      NaN           8\n",
       "98490        12  sheldon      NaN           8\n",
       "99780        17    other      NaN          14"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['dialogue'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pack the batch of sequences of variable length\n",
    "solution maybe: (not sure)<br>\n",
    "tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')<br>\n",
    "train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t[0]) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence,num_samples):\n",
    "    sent_list = list()\n",
    "    for k in range(0,num_samples):\n",
    "        sent_list.append(preprocess_sentence(dialogues[k]))\n",
    "    sent_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters='')\n",
    "    sent_tokenizer.fit_on_texts(sent_list)\n",
    "\n",
    "    tensor = sent_tokenizer.texts_to_sequences(sent_list)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                     padding='post',\n",
    "                                                     value=0)\n",
    "\n",
    "    return tensor, sent_tokenizer,sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor, tokenizer,sent_list = tokenize(dialogues,data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "electrical ----> 8652\n",
      "vegetarianism ----> 18815\n",
      "stwippers ----> 25038\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key,val in tokenizer.word_index.items():\n",
    "    count += 1\n",
    "    print (\"{0} ----> {1}\".format(key, val))\n",
    "    if count==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up a dictionary index:word\n",
    "index2word = {v: k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----> .\n",
      "2 ----> <sos>\n",
      "3 ----> <eos>\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for key,val in index2word.items():\n",
    "    count += 1\n",
    "    print (\"{0} ----> {1}\".format(key, val))\n",
    "    if count==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,     5,    22,  4010,   263,    14,   229,   402, 11626,\n",
       "          63,    24,     1,     3,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0], dtype=int32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<SOS> i m tellin ya that girl totally winked at me . <EOS>'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tensor,num_samples):\n",
    "    dialogues_list = list()\n",
    "    response_list = list()\n",
    "    for k in range(0,num_samples,2):\n",
    "        if(k+1 >= num_samples):\n",
    "            break\n",
    "        if episodes[k]==episodes[k+1]:\n",
    "            dialogue = tensor[k]\n",
    "#             pdb.set_trace()\n",
    "            response = tensor[k+1]\n",
    "            speaker = speakerid_list[k]\n",
    "            addressee = speakerid_list[k+1]\n",
    "            dialogues_list.append([dialogue,speaker])\n",
    "            response_list.append([response,addressee])\n",
    "#     print(dialogues_list)\n",
    "#     print(response_list)\n",
    "    return dialogues_list,response_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor,r_tensor = create_dataset(tensor,data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56123"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "262\n"
     ]
    }
   ],
   "source": [
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(r_tensor), max_length(d_tensor)\n",
    "print(max_length_targ)\n",
    "print(max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 1, 2, 4], [33, 11, 22, 44]]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arr1 = [1,2,3,4]\n",
    "# arr2 = [11,22,33,44]\n",
    "# shuffle(arr1,arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "tensor = shuffle(d_tensor,r_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor = tensor[0]\n",
    "r_tensor = tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44898 44898 11225 11225\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "d_tensor_train, d_tensor_val, r_tensor_train, r_tensor_val = train_test_split(d_tensor, r_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(d_tensor_train), len(r_tensor_train), len(d_tensor_val), len(r_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_train = [t[0] for t in d_tensor_train]\n",
    "dia_val = [t[0] for t in d_tensor_val]\n",
    "sid_train = [t[1] for t in d_tensor_train]\n",
    "sid_val = [t[1] for t in d_tensor_val]\n",
    "res_train = [t[0] for t in r_tensor_train]\n",
    "res_val = [t[0] for t in r_tensor_val]\n",
    "aid_train = [t[1] for t in r_tensor_train]\n",
    "aid_val = [t[1] for t in r_tensor_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(d_tensor_train)\n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = len(d_tensor_train)//BATCH_SIZE\n",
    "HIDDEN_SIZE = 1000\n",
    "NUM_LAYER = 4\n",
    "DROP_OUT = 0.2\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 512\n",
    "speakerNum = len(main_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dia_train,res_train, sid_train,aid_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, vocab_size,embedding_dim, num_layers=1, batch_size=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.input_size = embedding_dim\n",
    "            \n",
    "        self.lstm = tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout=DROP_OUT,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, d, hidden):\n",
    "        d = self.embedding(d)\n",
    "        # four layer train, 4 lstm\n",
    "        for k in range(num_layer):\n",
    "            output, hidden = self.lstm(d, initial_state = hidden)\n",
    "            d = output\n",
    "        return output, hidden_state\n",
    "\n",
    "    def initialize_hidden_state(self,batch_size=0):\n",
    "        if batch_size == 0: batch_size =self.batch_size\n",
    "        return tf.zeros((self.batch_size, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Feed(tf.keras.Model):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention_Feed, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(hidden_size)\n",
    "        self.W2 = tf.keras.layers.Dense(hidden_size)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "#         print(\"score.size():{0}\".format(score.shape))\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "#         print(\"values.size():{0}\".format(values.shape))\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "#         print(\"context_vector.size():{0}\".format(context_vector.shape))\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_user_vector(i_tensor,j_tensor):\n",
    "    size = s_tensor.shape[-1]\n",
    "    W1 = tf.keras.layers.Dense(size)\n",
    "    W2 = tf.keras.layers.Dense(size)\n",
    "    V_ij = tf.nn.tanh(W1(i_tensor) + W2(j_tensor))\n",
    "    return V_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, vocab_size,embedding_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.speaker_embedding = tf.keras.layers.Embedding(speakerNum, embedding_dim)\n",
    "        self.input_size = embedding_dim\n",
    "        self.output_size = vocab_size #vocabulary size           \n",
    "        self.lstm = tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = DORP_OUT,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(self.output_size)\n",
    "        \n",
    "        # attention feed on context\n",
    "        self.attention = Attention_Feed(self.hidden_size)\n",
    "\n",
    "    def call(self, x, hidden, enc_output,speaker_id,addressee_id=None):\n",
    "#         batch_size = x.size()[1]\n",
    "        \n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        features = self.embedding(x)\n",
    "        # personas\n",
    "        speaker = self.speaker_embedding(speaker_id)\n",
    "        if addressee_id is not None:\n",
    "            addressee = self.speaker_embedding(addressee_id)\n",
    "            v_ij = combine_user_vector(speaker,addressee)\n",
    "            features = tf.concat([features, v_ij], axis=-1)\n",
    "        else:\n",
    "            features = tf.concat([features, speaker], axis=-1)\n",
    "#         max_length = enc_output.size(0)  \n",
    "        r = tf.concat([tf.expand_dims(context_vector, 1), features], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the 4-layer LSTM\n",
    "        for k in range(num_layers):\n",
    "            output, state = self.lstm(r)\n",
    "            r = output\n",
    "\n",
    "        # Removes dimensions of size 1 from the shape of a tensor.\n",
    "        # output shape: (batch_size, 1, hidden_size) --> (batch_size *1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "#         output = output.squeeze(0)\n",
    "\n",
    "        # output shape == （batch_size, hidden_size)\n",
    "        output = tf.nn.log_softmax(self.fc(output),axis=1)\n",
    "\n",
    "        return output, state,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tf tutorial\n",
    "# loss = 0\n",
    "\n",
    "# with tf.GradientTape() as tape:\n",
    "#     enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "#     dec_hidden = enc_hidden\n",
    "\n",
    "#     dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "#     # Teacher forcing - feeding the target as the next input\n",
    "#     for t in range(1, targ.shape[1]):\n",
    "#         # passing enc_output to the decoder\n",
    "#         predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "#         loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "#         # using teacher forcing\n",
    "#         dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "#     batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "#     variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "#     gradients = tape.gradient(loss, variables)\n",
    "\n",
    "#     optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "#     return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(object):\n",
    "    def __init__(self,encoder,decoder,optimizer,index2word,num_layers=1):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.index2word = index2word\n",
    "        self.num_layers = num_layers\n",
    "        self.optimizer = optimizer\n",
    "    \n",
    "    def train_step(self,inp, targ, speaker_id,addressee_id=None):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output,enc_hidden = encoder(inp,enc_hidden)\n",
    "            dec_hidden = enc_hidden\n",
    "            dec_input = tf.expand_dims([tokenizer.word_index['<sos>']]*BATCH_SIZE,1)\n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                if addressee_id is not None:\n",
    "                    predictions, dec_hidden, _ = decoder(\n",
    "                        dec_input, dec_hidden,enc_output,speaker_id,addressee_id)\n",
    "                else:\n",
    "                    predictions, dec_hidden, _ = decoder(\n",
    "                        dec_input, dec_hidden,enc_output,speaker_id)\n",
    "                    \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "            \n",
    "            variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "\n",
    "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to modify\n",
    "def run_iter(epochs,isAddressee):\n",
    "    for e in range(epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        enc_hidden = encoder.initialize_hidden_state()\n",
    "        total_loss = 0\n",
    "\n",
    "        if isAddressee==True:\n",
    "            for (batch, (inp, targ,sid,aid)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "            batch_loss = train_step(inp, targ, enc_hidden,sid,aid)\n",
    "            total_loss += batch_loss\n",
    "\n",
    "            if batch % 100 == 0:\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                             batch,\n",
    "                                                             batch_loss.numpy()))\n",
    "        else:\n",
    "            for (batch, (inp, targ,_,sid)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "                batch_loss = train_step(inp, targ, enc_hidden,sid)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                                 batch,\n",
    "                                                                 batch_loss.numpy()))\n",
    "        # saving (checkpoint) the model every 2 epochs\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                          total_loss / steps_per_epoch))\n",
    "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
