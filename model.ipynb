{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "friends = 'friends.csv'\n",
    "bigbang = 'bigbang.csv'\n",
    "\n",
    "friends_df = pd.read_csv(path+friends)\n",
    "bigbang_df = pd.read_csv(path+bigbang)\n",
    "\n",
    "# need to drop sentence which are NA (because they represents some action of the characters)\n",
    "na_index = bigbang_df[bigbang_df['dialogue'].isna()].index\n",
    "bigbang_df.drop(index = na_index,inplace=True )\n",
    "\n",
    "df = pd.concat([friends_df, bigbang_df], ignore_index=True, sort=False)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amy': 12,\n",
       " 'bernadette': 13,\n",
       " 'chandler': 3,\n",
       " 'howard': 10,\n",
       " 'joey': 1,\n",
       " 'leonard': 7,\n",
       " 'monica': 4,\n",
       " 'other': 14,\n",
       " 'penny': 9,\n",
       " 'phoebe': 6,\n",
       " 'rachel': 2,\n",
       " 'raj': 11,\n",
       " 'ross': 5,\n",
       " 'sheldon': 8}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_c = ['joey','rachel','chandler','monica','ross','phoebe','leonard',\n",
    "          'sheldon','penny','howard','raj','amy','bernadette','other']\n",
    "speakers_ind=dict()\n",
    "for ind, c in enumerate(main_c,1):\n",
    "    speakers_ind[c] = ind\n",
    "    \n",
    "df['speaker_id'] = df['speakers'].apply(lambda x: speakers_ind[x] - 1)\n",
    "speakerid_list = list(df['speaker_id'])\n",
    "speakers = list(df['speakers'])\n",
    "dialogues = list(df['dialogue'])\n",
    "episodes = list(df['episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<SOS> ' + w + ' <EOS>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> i m tellin ya that girl totally winked at me . <EOS>\n",
      "b'<SOS> i m tellin ya that girl totally winked at me . <EOS>'\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(dialogues[0]))\n",
    "print(preprocess_sentence(dialogues[0]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pack the batch of sequences of variable length\n",
    "solution maybe: (not sure)<br>\n",
    "tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')<br>\n",
    "train_dataset.padded_batch(BATCH_SIZE, train_dataset.output_shapes)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t[0]) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence,num_samples):\n",
    "    sent_list = list()\n",
    "    for k in range(0,num_samples):\n",
    "        sent_list.append(preprocess_sentence(dialogues[k]))\n",
    "    sent_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters='')\n",
    "    sent_tokenizer.fit_on_texts(sent_list)\n",
    "\n",
    "    tensor = sent_tokenizer.texts_to_sequences(sent_list)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                     padding='post',\n",
    "                                                     value=0)\n",
    "\n",
    "    return tensor, sent_tokenizer,sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor, tokenizer,sent_list = tokenize(dialogues,data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convertible ----> 18949\n",
      "deciding ----> 3934\n",
      "concentric ----> 8118\n"
     ]
    }
   ],
   "source": [
    "# just to show if tokenizer work well\n",
    "# count = 0\n",
    "# for key,val in tokenizer.word_index.items():\n",
    "#     count += 1\n",
    "#     print (\"{0} ----> {1}\".format(key, val))\n",
    "#     if count==3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build up a dictionary index:word\n",
    "index2word = {v: k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ----> .\n",
      "2 ----> <sos>\n",
      "3 ----> <eos>\n"
     ]
    }
   ],
   "source": [
    "# just to show if reverse work well\n",
    "# count = 0\n",
    "# for key,val in index2word.items():\n",
    "#     count += 1\n",
    "#     print (\"{0} ----> {1}\".format(key, val))\n",
    "#     if count==3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tensor,num_samples):\n",
    "    dialogues_list = list()\n",
    "    response_list = list()\n",
    "    for k in range(0,num_samples,2):\n",
    "        if(k+1 >= num_samples):\n",
    "            break\n",
    "        if episodes[k]==episodes[k+1]:\n",
    "            dialogue = tensor[k]\n",
    "#             pdb.set_trace()\n",
    "            response = tensor[k+1]\n",
    "            addressee = tf.convert_to_tensor(speakerid_list[k])\n",
    "            speaker = tf.convert_to_tensor(speakerid_list[k+1])\n",
    "            dialogues_list.append([dialogue,addressee])\n",
    "            response_list.append([response,speaker])\n",
    "#     print(dialogues_list)\n",
    "#     print(response_list)\n",
    "    return dialogues_list,response_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor,r_tensor = create_dataset(tensor,data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "262\n"
     ]
    }
   ],
   "source": [
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(r_tensor), max_length(d_tensor)\n",
    "print(max_length_targ)\n",
    "print(max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arr1 = [1,2,3,4]\n",
    "# arr2 = [11,22,33,44]\n",
    "# shuffle(arr1,arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "tensor = shuffle(d_tensor,r_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor = tensor[0]\n",
    "r_tensor = tensor[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44898 44898 11225 11225\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation sets using an 80-20 split\n",
    "d_tensor_train, d_tensor_val, r_tensor_train, r_tensor_val = train_test_split(d_tensor, r_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(d_tensor_train), len(r_tensor_train), len(d_tensor_val), len(r_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_train = [t[0] for t in d_tensor_train]\n",
    "dia_val = [t[0] for t in d_tensor_val]\n",
    "aid_train = [t[1] for t in d_tensor_train]\n",
    "aid_val = [t[1] for t in d_tensor_val]\n",
    "res_train = [t[0] for t in r_tensor_train]\n",
    "res_val = [t[0] for t in r_tensor_val]\n",
    "sid_train = [t[1] for t in r_tensor_train]\n",
    "sid_val = [t[1] for t in r_tensor_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(d_tensor_train)\n",
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = len(d_tensor_train)//BATCH_SIZE\n",
    "HIDDEN_SIZE = 1000\n",
    "NUM_LAYER = 4\n",
    "DROP_OUT = 0.2\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 512\n",
    "speakerNum = len(main_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dia_train,res_train, sid_train,aid_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16, 262]), TensorShape([16, 262]))"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch,example_sid_batch, example_aid_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, vocab_size,embedding_dim, num_layers=1, batch_size=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.input_size = embedding_dim            \n",
    "        self.lstm_1 = tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout=DROP_OUT,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.lstms = []\n",
    "        for k in range(self.num_layers - 1):\n",
    "            self.lstms.append(tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout=DROP_OUT))\n",
    "    def call(self, d, init_state):\n",
    "        d = self.embedding(d)\n",
    "#         print ('Encoder input shape: {}'.format(d.shape))\n",
    "        output, hidden,c = self.lstm_1(d, initial_state = init_state)\n",
    "        init_state = [hidden, c]\n",
    "        # four layer train, 4 lstm\n",
    "        for k in range(self.num_layers - 1):\n",
    "            output, hidden,c = self.lstms[k](output, initial_state = init_state)\n",
    "            init_state = [hidden, c]\n",
    "        return output, hidden,c\n",
    "\n",
    "    def initialize_hidden_state(self,batch_size=0):\n",
    "        if batch_size == 0: batch_size =self.batch_size\n",
    "        init_hidden = tf.zeros((self.batch_size, self.hidden_size))\n",
    "        init_c = tf.zeros((self.batch_size, self.hidden_size))\n",
    "        return [init_hidden,init_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (16, 262, 1000)\n",
      "Encoder Hidden state shape: (batch size, units) (16, 1000)\n"
     ]
    }
   ],
   "source": [
    "# test encoder\n",
    "encoder = Encoder(HIDDEN_SIZE,vocab_size, embedding_dim, NUM_LAYER, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_init_state = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden,sample_c = encoder(example_input_batch, sample_init_state)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Feed(tf.keras.Model):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention_Feed, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(hidden_size)\n",
    "        self.W2 = tf.keras.layers.Dense(hidden_size)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "#         print(\"score.size():{0}\".format(score.shape))\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "#         print(\"values.size():{0}\".format(values.shape))\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "#         print(\"context_vector.size():{0}\".format(context_vector.shape))\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (16, 1000)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (16, 262, 1)\n"
     ]
    }
   ],
   "source": [
    "# test attention_feed\n",
    "attention_layer = Attention_Feed(HIDDEN_SIZE)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_user_vector(i_em,j_em):\n",
    "    # size is equal to the number of \n",
    "    size = i_em.shape[-1]\n",
    "    W1 = tf.keras.layers.Dense(size)\n",
    "    W2 = tf.keras.layers.Dense(size)\n",
    "    V_ij = tf.nn.tanh(W1(i_em) + W2(j_em))\n",
    "    return V_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, vocab_size,embedding_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.speaker_embedding = tf.keras.layers.Embedding(speakerNum, embedding_dim)\n",
    "        self.input_size = embedding_dim\n",
    "        self.output_size = vocab_size #vocabulary size           \n",
    "        self.lstm_1 = tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = DROP_OUT,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.lstms = []\n",
    "        for k in range(self.num_layers - 1):\n",
    "            self.lstms.append(tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout=DROP_OUT))\n",
    "        self.fc = tf.keras.layers.Dense(self.output_size)\n",
    "        \n",
    "        # attention feed on context\n",
    "        self.attention = Attention_Feed(self.hidden_size)\n",
    "\n",
    "    def call(self, x, enc_output,init_state,speaker_id,addressee_id=None):\n",
    "#         batch_size = x.size()[1]\n",
    "        hidden = init_state[0]\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        features = self.embedding(x)\n",
    "        # personas\n",
    "        speaker = self.speaker_embedding(speaker_id)\n",
    "#         print(\"Speaker shape: {}\".format(speaker.shape))\n",
    "#         print(\"finish speaker embedding\")\n",
    "        if addressee_id is not None:\n",
    "            addressee = self.speaker_embedding(addressee_id)\n",
    "            v_ij = combine_user_vector(speaker,addressee)\n",
    "            features = tf.concat([features,tf.expand_dims(v_ij,1)], axis=-1)\n",
    "        else:\n",
    "            features = tf.concat([features, tf.expand_dims(speaker, 1)], axis=-1)\n",
    "#         max_length = enc_output.size(0)  \n",
    "        r = tf.concat([tf.expand_dims(context_vector, 1), features], axis=-1)\n",
    "#         print(\"finish concatenate\")\n",
    "        \n",
    "        # passing the concatenated vector to the 4-layer LSTM\n",
    "        output, hidden,c = self.lstm_1(r,initial_state = init_state)\n",
    "        init_state = [hidden, c]\n",
    "        for k in range(self.num_layers - 1):\n",
    "            output, state,c = self.lstms[k](output,initial_state = init_state)\n",
    "            init_state = [hidden, c]\n",
    "        \n",
    "#         print(\"finish 4-layer LSTM\")\n",
    "        # Removes dimensions of size 1 from the shape of a tensor.\n",
    "        # output shape: (batch_size, 1, hidden_size) --> (batch_size *1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "#         output = output.squeeze(0)\n",
    "\n",
    "        # output shape == （batch_size, hidden_size)\n",
    "        output = tf.nn.log_softmax(self.fc(output),axis=1)\n",
    "#         print(\"finish all\")\n",
    "        return output, state,c,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (16, 26028)\n"
     ]
    }
   ],
   "source": [
    "# test decoder\n",
    "decoder = Decoder(HIDDEN_SIZE,vocab_size, embedding_dim,NUM_LAYER)\n",
    "init_state = [sample_hidden,sample_c]\n",
    "sp = tf.convert_to_tensor([1]*BATCH_SIZE)\n",
    "sample_decoder_output, _, _,_ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                 sample_output,init_state,sp)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train(object):\n",
    "    def __init__(self,encoder,decoder,optimizer,tokenizer,num_layers=1):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "#         self.index2word = index2word\n",
    "#         self.num_layers = num_layers\n",
    "        \n",
    "    def loss_function(self,real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = self.loss_object(real, pred)\n",
    "\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "\n",
    "        return tf.reduce_mean(loss_)\n",
    "    \n",
    "#     @tf.function\n",
    "    def train_step(self,inp, targ, enc_hidden,speaker_id,addressee_id=None):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output,enc_hidden,enc_c = self.encoder(inp,enc_hidden)\n",
    "            dec_init_state = [enc_hidden,enc_c]\n",
    "            dec_input = tf.expand_dims([self.tokenizer.word_index['<sos>']]*BATCH_SIZE,1)\n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                if addressee_id is not None:\n",
    "                    predictions, dec_hidden,dec_c, _ = self.decoder(\n",
    "                        dec_input,enc_output, dec_init_state,speaker_id,addressee_id)\n",
    "                else:\n",
    "                    predictions, dec_hidden, dec_c,_ = self.decoder(\n",
    "                        dec_input,enc_output, dec_init_state,speaker_id)\n",
    "                dec_init_state = [dec_hidden,dec_c]\n",
    "#                 print(\"targ[:,t] shape: {}\".format(targ[:,t].shape))\n",
    "                loss += self.loss_function(targ[:,t], predictions)\n",
    "\n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "#             print(\"batch_loss: {}\".format(batch_loss))\n",
    "            variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "#             print(\"Get gradient:{}\".format(type(gradients)))\n",
    "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "#             print(\"finish all.\")\n",
    "            return batch_loss\n",
    "\n",
    "    def run_iter(self,epochs,isAddressee,steps_per_epoch,checkpoint,checkpoint_prefix):\n",
    "        for e in range(epochs):\n",
    "            start = time.time()\n",
    "\n",
    "            enc_hidden = self.encoder.initialize_hidden_state()\n",
    "            total_loss = 0\n",
    "\n",
    "            \n",
    "            for (batch, (inp, targ,sid,aid)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "                if isAddressee==True:\n",
    "                    batch_loss = self.train_step(inp, targ, enc_hidden,sid,aid)\n",
    "                else:\n",
    "                    batch_loss = self.train_step(inp, targ, enc_hidden,sid)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "#                 if batch % 100 == 0:\n",
    "#                     print('Epoch {} Batch {} Loss {:.4f}'.format(e + 1,\n",
    "#                                                                  batch,\n",
    "#                                                                  batch_loss.numpy()))\n",
    "                #just for test\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(e + 1,\n",
    "                                                                 batch,\n",
    "                                                                 batch_loss.numpy()))\n",
    "                # just for test\n",
    "                if batch==10: break\n",
    "            \n",
    "            # saving (checkpoint) the model every 2 epochs\n",
    "            if (e + 1) % 2 == 0:\n",
    "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "            print('Epoch {} Loss {:.4f}'.format(e + 1,\n",
    "                                              total_loss / steps_per_epoch))\n",
    "            print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './persona_training_checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"sam_test\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_input_batch\n",
    "# example_target_batch\n",
    "# example_sid_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder input shape: (16, 262, 512)\n",
      "batch_loss: 0.5044651627540588\n",
      "finish all.\n"
     ]
    }
   ],
   "source": [
    "# just to test if train_step function work well\n",
    "train_nn = Train(encoder,decoder,optimizer,tokenizer)\n",
    "enc_hidden = encoder.initialize_hidden_state()\n",
    "train_nn.train_step(example_input_batch,example_target_batch,enc_hidden,example_sid_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.3854\n",
      "Epoch 1 Batch 1 Loss 0.4234\n",
      "Epoch 1 Batch 2 Loss 0.2681\n",
      "Epoch 1 Batch 3 Loss 0.3121\n",
      "Epoch 1 Batch 4 Loss 0.3797\n",
      "Epoch 1 Batch 5 Loss 0.6581\n",
      "Epoch 1 Batch 6 Loss 0.4139\n",
      "Epoch 1 Batch 7 Loss 0.3326\n",
      "Epoch 1 Batch 8 Loss 0.3914\n",
      "Epoch 1 Batch 9 Loss 0.4585\n",
      "Epoch 1 Batch 10 Loss 0.3880\n",
      "Epoch 1 Loss 0.0016\n",
      "Time taken for 1 epoch 2253.0359179973602 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.3814\n",
      "Epoch 2 Batch 1 Loss 0.3554\n",
      "Epoch 2 Batch 2 Loss 0.3514\n",
      "Epoch 2 Batch 3 Loss 0.2453\n",
      "Epoch 2 Batch 4 Loss 0.2790\n",
      "Epoch 2 Batch 5 Loss 0.3694\n",
      "Epoch 2 Batch 6 Loss 0.2616\n",
      "Epoch 2 Batch 7 Loss 0.3088\n",
      "Epoch 2 Batch 8 Loss 0.3827\n",
      "Epoch 2 Batch 9 Loss 0.2726\n",
      "Epoch 2 Batch 10 Loss 0.3689\n",
      "Epoch 2 Loss 0.0013\n",
      "Time taken for 1 epoch 2185.4651601314545 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_nn = Train(encoder,decoder,optimizer,tokenizer)\n",
    "train_nn.run_iter(EPOCHS,False,steps_per_epoch,checkpoint,checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(train_nn,inp,targ,speaker_id,addressee_id=None,batch_size=BATCH_SIZE):\n",
    "    loss = 0\n",
    "    inp = np.asarray(inp)\n",
    "    targ = np.asarray(targ)\n",
    "    speaker_id = np.asarray(speaker_id)\n",
    "    if addressee_id is not None:\n",
    "        addressee_id = np.asarray(addressee_id)\n",
    "#     print(\"test targ[:,1].shape: {}\".format(targ[:,1].shape))\n",
    "    val_size = targ.shape[0]\n",
    "    print(\"val_size: {}\".format(val_size))\n",
    "    num_each_batch = (int)(np.floor(val_size/batch_size))\n",
    "    remaining_num = val_size - num_each_batch * batch_size\n",
    "    if remaining_num == 0:\n",
    "        remaining_num = batch_size\n",
    "    for k in range(0,val_size,batch_size):\n",
    "        start = time.time()\n",
    "        batch_loss = 0\n",
    "        if (k+batch_size)>=val_size:\n",
    "            print(\"k now +batch_size>=val_size: {}\".format(k))\n",
    "            inputs = inp[k:]\n",
    "            targs = targ[k:]\n",
    "            s_id = speaker_id[k:]\n",
    "            print(\"s_id.shape: {}\".format(s_id.shape))\n",
    "            if addressee_id is not None:\n",
    "                a_id = addressee_id[k:]\n",
    "            enc_hidden = [tf.zeros((remaining_num, HIDDEN_SIZE)),tf.zeros((remaining_num, HIDDEN_SIZE))]\n",
    "            \n",
    "            enc_out, enc_hidden,enc_c = train_nn.encoder(inputs, enc_hidden)\n",
    "            dec_init_state = [enc_hidden,enc_c]\n",
    "            dec_input = tf.expand_dims([train_nn.tokenizer.word_index['<sos>']]*remaining_num, 1)\n",
    "        else:\n",
    "            print(\"k now: {}\".format(k))\n",
    "            inputs = inp[k:k+batch_size]\n",
    "            targs = targ[k:k+batch_size]\n",
    "            s_id = speaker_id[k:k+batch_size]\n",
    "            print(\"s_id.shape: {}\".format(s_id.shape))\n",
    "            if addressee_id is not None:\n",
    "                a_id = addressee_id[k:k+batch_size]\n",
    "            \n",
    "            enc_hidden = [tf.zeros((batch_size, HIDDEN_SIZE)),tf.zeros((batch_size, HIDDEN_SIZE))]\n",
    "            enc_out, enc_hidden,enc_c = train_nn.encoder(inputs, enc_hidden)\n",
    "            dec_init_state = [enc_hidden,enc_c]\n",
    "            dec_input = tf.expand_dims([train_nn.tokenizer.word_index['<sos>']]*batch_size, 1)\n",
    "\n",
    "        for t in range(targ.shape[1]):\n",
    "            if addressee_id is not None:\n",
    "                predictions, dec_hidden, dec_c,_ = train_nn.decoder(dec_input,enc_out, dec_init_state,s_id,a_id)\n",
    "            else:\n",
    "                predictions, dec_hidden, dec_c,_ = train_nn.decoder(dec_input,enc_out, dec_init_state,s_id)\n",
    "            \n",
    "            # use the max prob one in each sentence in the batch\n",
    "            predicted_id = tf.argmax(predictions,axis=1).numpy()\n",
    "            dec_init_state = [dec_hidden,dec_c]\n",
    "            batch_loss += train_nn.loss_function(targs[:,t], predictions)\n",
    "#             print(\"test predictions[0]: {}\".format(predictions[0]))\n",
    "#             predictions = np.asarray(predictions)\n",
    "            predicted = tf.constant(predicted_id)\n",
    "            dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        \n",
    "        if (k+batch_size)>=val_size:\n",
    "            loss += batch_loss/batch_size\n",
    "        else:\n",
    "            loss += batch_loss/remaining_num\n",
    "        \n",
    "        print('batch {} Loss {:.4f}'.format(k/batch_size + 1,\n",
    "                                              loss))\n",
    "        print('Time taken {} sec\\n'.format(time.time() - start))  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 128\n",
    "size = 2*batch_sz\n",
    "sample_input_val = dia_val[:size]\n",
    "sample_targ_val = res_val[:size]\n",
    "sample_sid_val = sid_val[:size]\n",
    "sample_aid_val = aid_val[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_size: 256\n",
      "k now: 0\n",
      "s_id.shape: (128,)\n",
      "batch 1.0 Loss 0.8863\n",
      "Time taken 188.5863161087036 sec\n",
      "\n",
      "k now +batch_size>=val_size: 128\n",
      "s_id.shape: (128,)\n",
      "batch 2.0 Loss 1.8059\n",
      "Time taken 193.88308095932007 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=8252833, shape=(), dtype=float32, numpy=1.8058624>"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation(train_nn,sample_input_val,sample_targ_val,sample_sid_val,batch_size = batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
