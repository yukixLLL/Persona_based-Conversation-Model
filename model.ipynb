{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/'\n",
    "friends = 'friends.csv'\n",
    "bigbang = 'bigbang.csv'\n",
    "\n",
    "friends_df = pd.read_csv(path+friends)\n",
    "bigbang_df = pd.read_csv(path+bigbang)\n",
    "\n",
    "# need to drop sentence which are NA (because they represents some action of the characters)\n",
    "na_index = bigbang_df[bigbang_df['dialogue'].isna()].index\n",
    "bigbang_df.drop(index = na_index,inplace=True )\n",
    "\n",
    "df = pd.concat([friends_df, bigbang_df], ignore_index=True, sort=False)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_c = ['joey','rachel','chandler','monica','ross','phoebe','leonard',\n",
    "          'sheldon','penny','howard','raj','amy','bernadette','other']\n",
    "speakers_ind=dict()\n",
    "for ind, c in enumerate(main_c,1):\n",
    "    speakers_ind[c] = ind\n",
    "    \n",
    "df['speaker_id'] = df['speakers'].apply(lambda x: speakers_ind[x] - 1)\n",
    "speakerid_list = list(df['speaker_id'])\n",
    "speakers = list(df['speakers'])\n",
    "dialogues = list(df['dialogue'])\n",
    "episodes = list(df['episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112700\n"
     ]
    }
   ],
   "source": [
    "data_size = len(df)\n",
    "MAXLEN = 100\n",
    "print(data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<SOS> ' + w + ' <EOS>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SOS> i m tellin ya that girl totally winked at me . <EOS>\n",
      "b'<SOS> i m tellin ya that girl totally winked at me . <EOS>'\n"
     ]
    }
   ],
   "source": [
    "print(preprocess_sentence(dialogues[0]))\n",
    "print(preprocess_sentence(dialogues[0]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t[0]) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence,num_samples):\n",
    "    sent_list = list()\n",
    "    for k in range(0,num_samples):\n",
    "        sent_list.append(preprocess_sentence(dialogues[k]))\n",
    "    sent_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters='')\n",
    "    sent_tokenizer.fit_on_texts(sent_list)\n",
    "\n",
    "    tensor = sent_tokenizer.texts_to_sequences(sent_list)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                    maxlen=MAXLEN,\n",
    "                                                     padding='post',\n",
    "                                                    truncating='post',\n",
    "                                                     value=0)\n",
    "\n",
    "    return tensor, sent_tokenizer,sent_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to show if tokenizer work well\n",
    "# count = 0\n",
    "# for key,val in tokenizer.word_index.items():\n",
    "#     count += 1\n",
    "#     print (\"{0} ----> {1}\".format(key, val))\n",
    "#     if count==3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor, tokenizer,sent_list = tokenize(dialogues,data_size)\n",
    "# build up a dictionary index:word\n",
    "index2word = {v: k for k, v in tokenizer.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to show if reverse work well\n",
    "# count = 0\n",
    "# for key,val in index2word.items():\n",
    "#     count += 1\n",
    "#     print (\"{0} ----> {1}\".format(key, val))\n",
    "#     if count==3:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tensor,num_samples):\n",
    "    dialogues_list = list()\n",
    "    response_list = list()\n",
    "    for k in range(0,num_samples):\n",
    "        if(k+1 >= num_samples):\n",
    "            break\n",
    "        if episodes[k]==episodes[k+1]:\n",
    "            dialogue = tensor[k]\n",
    "#             pdb.set_trace()\n",
    "            response = tensor[k+1]\n",
    "            addressee = tf.convert_to_tensor(speakerid_list[k])\n",
    "            speaker = tf.convert_to_tensor(speakerid_list[k+1])\n",
    "            dialogues_list.append([dialogue,addressee])\n",
    "            response_list.append([response,speaker])\n",
    "#     print(dialogues_list)\n",
    "#     print(response_list)\n",
    "    return dialogues_list,response_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tensor,r_tensor = create_dataset(tensor,data_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = max_length(r_tensor), max_length(d_tensor)\n",
    "print(max_length_targ)\n",
    "print(max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112244"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89795 89795 22449 22449\n"
     ]
    }
   ],
   "source": [
    "# shuffle\n",
    "tensor = shuffle(d_tensor,r_tensor)\n",
    "d_tensor = tensor[0]\n",
    "r_tensor = tensor[1]\n",
    "\n",
    "# Creating training and validation sets using an 80-20 split\n",
    "d_tensor_train, d_tensor_val, r_tensor_train, r_tensor_val = train_test_split(d_tensor, r_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(d_tensor_train), len(r_tensor_train), len(d_tensor_val), len(r_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dia_train = [t[0] for t in d_tensor_train]\n",
    "dia_val = [t[0] for t in d_tensor_val]\n",
    "aid_train = [t[1] for t in d_tensor_train]\n",
    "aid_val = [t[1] for t in d_tensor_val]\n",
    "res_train = [t[0] for t in r_tensor_train]\n",
    "res_val = [t[0] for t in r_tensor_val]\n",
    "sid_train = [t[1] for t in r_tensor_train]\n",
    "sid_val = [t[1] for t in r_tensor_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(d_tensor_train)\n",
    "# remember to change BATCH_SIZE, 16 just for test\n",
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = len(d_tensor_train)//BATCH_SIZE  + 1\n",
    "HIDDEN_SIZE = 1000\n",
    "NUM_LAYER = 4\n",
    "DROP_OUT = 0.2\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 512\n",
    "speakerNum = len(main_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5613"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dia_train,res_train, sid_train,aid_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([16, 100]), TensorShape([16, 100]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch,example_sid_batch, example_aid_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, vocab_size,embedding_dim, num_layers=1, batch_size=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.input_size = embedding_dim            \n",
    "        self.lstm_1 = tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout=DROP_OUT,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.lstms = []\n",
    "        for k in range(self.num_layers - 1):\n",
    "            self.lstms.append(tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout=DROP_OUT))\n",
    "    def call(self, d, init_state):\n",
    "        d = self.embedding(d)\n",
    "#         print ('Encoder input shape: {}'.format(d.shape))\n",
    "        output, hidden,c = self.lstm_1(d, initial_state = init_state)\n",
    "        init_state = [hidden, c]\n",
    "        # four layer train, 4 lstm\n",
    "        for k in range(self.num_layers - 1):\n",
    "            output, hidden,c = self.lstms[k](output, initial_state = init_state)\n",
    "            init_state = [hidden, c]\n",
    "        return output, hidden,c\n",
    "\n",
    "    def initialize_hidden_state(self,batch_size=0):\n",
    "        if batch_size == 0: batch_size =self.batch_size\n",
    "        init_hidden = tf.zeros((batch_size, self.hidden_size))\n",
    "        init_c = tf.zeros((batch_size, self.hidden_size))\n",
    "        return [init_hidden,init_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (16, 100, 1000)\n",
      "Encoder Hidden state shape: (batch size, units) (16, 1000)\n"
     ]
    }
   ],
   "source": [
    "# test encoder\n",
    "encoder = Encoder(HIDDEN_SIZE,vocab_size, embedding_dim, NUM_LAYER, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_init_state = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden,sample_c = encoder(example_input_batch, sample_init_state)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_Feed(tf.keras.Model):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention_Feed, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(hidden_size)\n",
    "        self.W2 = tf.keras.layers.Dense(hidden_size)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "#         print(\"score.size():{0}\".format(score.shape))\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "#         print(\"values.size():{0}\".format(values.shape))\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "#         print(\"context_vector.size():{0}\".format(context_vector.shape))\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (16, 1000)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (16, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# test attention_feed\n",
    "attention_layer = Attention_Feed(HIDDEN_SIZE)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_user_vector(i_em,j_em):\n",
    "    # size is equal to the number of \n",
    "    size = i_em.shape[-1]\n",
    "    W1 = tf.keras.layers.Dense(size)\n",
    "    W2 = tf.keras.layers.Dense(size)\n",
    "    V_ij = tf.nn.tanh(W1(i_em) + W2(j_em))\n",
    "    return V_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, vocab_size,embedding_dim, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.speaker_embedding = tf.keras.layers.Embedding(speakerNum, embedding_dim)\n",
    "        self.input_size = embedding_dim\n",
    "        self.output_size = vocab_size #vocabulary size           \n",
    "        self.lstm_1 = tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout = DROP_OUT,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.lstms = []\n",
    "        for k in range(self.num_layers - 1):\n",
    "            self.lstms.append(tf.keras.layers.LSTM(self.hidden_size,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       dropout=DROP_OUT))\n",
    "        self.fc = tf.keras.layers.Dense(self.output_size)\n",
    "        \n",
    "        # attention feed on context\n",
    "        self.attention = Attention_Feed(self.hidden_size)\n",
    "\n",
    "    def call(self, x, enc_output,init_state,speaker_id,addressee_id=None):\n",
    "#         batch_size = x.size()[1]\n",
    "        hidden = init_state[0]\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        features = self.embedding(x)\n",
    "        # personas\n",
    "        speaker = self.speaker_embedding(speaker_id)\n",
    "#         print(\"Speaker shape: {}\".format(speaker.shape))\n",
    "#         print(\"finish speaker embedding\")\n",
    "        if addressee_id is not None:\n",
    "#             print(\"detect addressee\")\n",
    "            addressee = self.speaker_embedding(addressee_id)\n",
    "            v_ij = combine_user_vector(speaker,addressee)\n",
    "            features = tf.concat([features,tf.expand_dims(v_ij,1)], axis=-1)\n",
    "        else:\n",
    "            features = tf.concat([features, tf.expand_dims(speaker, 1)], axis=-1)\n",
    "#         max_length = enc_output.size(0)  \n",
    "        r = tf.concat([tf.expand_dims(context_vector, 1), features], axis=-1)\n",
    "#         print(\"finish concatenate\")\n",
    "        \n",
    "        # passing the concatenated vector to the 4-layer LSTM\n",
    "        output, hidden,c = self.lstm_1(r,initial_state = init_state)\n",
    "        init_state = [hidden, c]\n",
    "        for k in range(self.num_layers - 1):\n",
    "            output, state,c = self.lstms[k](output,initial_state = init_state)\n",
    "            init_state = [hidden, c]\n",
    "        \n",
    "#         print(\"finish 4-layer LSTM\")\n",
    "        # Removes dimensions of size 1 from the shape of a tensor.\n",
    "        # output shape: (batch_size, 1, hidden_size) --> (batch_size *1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "#         output = output.squeeze(0)\n",
    "\n",
    "        # output shape == （batch_size, hidden_size)\n",
    "        output = tf.nn.log_softmax(self.fc(output),axis=1)\n",
    "#         print(\"finish all\")\n",
    "        return output, state,c,attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test decoder\n",
    "decoder1 = Decoder(HIDDEN_SIZE,vocab_size, embedding_dim,NUM_LAYER)\n",
    "init_state = [sample_hidden,sample_c]\n",
    "sp = tf.convert_to_tensor([1]*BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (16, 26028)\n"
     ]
    }
   ],
   "source": [
    "# speaker model\n",
    "sample_decoder_output1, _, _,_ = decoder1(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                 sample_output,init_state,sp)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output1.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (16, 26028)\n"
     ]
    }
   ],
   "source": [
    "# speaker addressee model\n",
    "decoder2 = Decoder(HIDDEN_SIZE,vocab_size, embedding_dim,NUM_LAYER)\n",
    "add = tf.convert_to_tensor([2]*BATCH_SIZE)\n",
    "sample_decoder_output2, _, _,_ = decoder2(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                 sample_output,init_state,sp,add)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_real_word(real):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "#     pdb.set_trace()\n",
    "    word_per_line = tf.math.reduce_sum(mask,1)\n",
    "    return word_per_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class Train(object):\n",
    "    def __init__(self,encoder,decoder,optimizer,tokenizer,num_layers=1):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=True, reduction='none')\n",
    "#         self.index2word = index2word\n",
    "#         self.num_layers = num_layers\n",
    "        \n",
    "    def loss_function(self,real, pred):\n",
    "        # loss of every word\n",
    "        # true word mask\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = self.loss_object(real, pred)\n",
    "        \n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ = loss_ * mask\n",
    "        return loss_\n",
    "    \n",
    "#     @tf.function\n",
    "    def train_step(self,inp, targ, enc_hidden,speaker_id,batch_size=BATCH_SIZE, addressee_id=None):\n",
    "        loss = 0\n",
    "        word_per_line = count_real_word(targ)\n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output,enc_hidden,enc_c = self.encoder(inp,enc_hidden)\n",
    "            dec_init_state = [enc_hidden,enc_c]\n",
    "            dec_input = tf.expand_dims([self.tokenizer.word_index['<sos>']]*batch_size,1)\n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                if addressee_id is not None:\n",
    "#                     print(\"detect addressee\")\n",
    "                    predictions, dec_hidden,dec_c, _ = self.decoder(\n",
    "                        dec_input,enc_output, dec_init_state,speaker_id,addressee_id)\n",
    "                else:\n",
    "                    predictions, dec_hidden, dec_c,_ = self.decoder(\n",
    "                        dec_input,enc_output, dec_init_state,speaker_id)\n",
    "                dec_init_state = [dec_hidden,dec_c]\n",
    "#                 loss += self.loss_function(targ[:,t], predictions)\n",
    "                loss_ = self.loss_function(targ[:, t], predictions)\n",
    "                loss  += tf.math.reduce_sum(loss_/word_per_line)\n",
    "\n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "#             print(\"batch_loss: {}\".format(batch_loss))\n",
    "            variables = self.encoder.trainable_variables + self.decoder.trainable_variables\n",
    "\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "#             print(\"Get gradient:{}\".format(type(gradients)))\n",
    "            self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "#             print(\"finish all.\")\n",
    "            return batch_loss\n",
    "\n",
    "    def run_iter(self,epochs,isAddressee,steps_per_epoch,checkpoint,checkpoint_prefix):\n",
    "        for e in range(epochs):\n",
    "            start = time.time()\n",
    "\n",
    "            enc_hidden = self.encoder.initialize_hidden_state()\n",
    "            total_loss = 0\n",
    "\n",
    "            \n",
    "            for (batch, (inp, targ,sid,aid)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "                # drop_reminder = False\n",
    "                batch_sz =targ.shape[0]\n",
    "                # just to test the last batch\n",
    "#                 if batch_sz == BATCH_SIZE:\n",
    "#                     continue\n",
    "\n",
    "#                 print(\"batch_size: {}\".format(batch_sz))\n",
    "                enc_hidden = self.encoder.initialize_hidden_state(batch_sz)\n",
    "#                 print(\"enc_hidden.shape: {}\".format(enc_hidden[0].shape))\n",
    "                if isAddressee==True:\n",
    "                    batch_loss = self.train_step(inp, targ, enc_hidden,sid,batch_sz,aid)\n",
    "                else:\n",
    "                    batch_loss = self.train_step(inp, targ, enc_hidden,sid,batch_sz)\n",
    "                total_loss += batch_loss\n",
    "\n",
    "#                 if batch % 100 == 0:\n",
    "#                     print('Epoch {} Batch {} Loss {:.4f}'.format(e + 1,\n",
    "#                                                                  batch,\n",
    "#                                                                  batch_loss.numpy()))\n",
    "                #just for test\n",
    "                print('Epoch {} Batch {} Loss {:.4f}'.format(e + 1,\n",
    "                                                                 batch,\n",
    "                                                                 batch_loss.numpy()))\n",
    "                # just for test\n",
    "                if batch==3: break\n",
    "            \n",
    "            # saving (checkpoint) the model every 2 epochs\n",
    "            if (e + 1) % 2 == 0:\n",
    "                checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "#             print('Epoch {} Loss {:.4f}'.format(e + 1,\n",
    "#                                               total_loss / steps_per_epoch))\n",
    "            # just for test\n",
    "            print('Epoch {} Loss {:.4f}'.format(e + 1,\n",
    "                                              total_loss / 3))\n",
    "            print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './persona_training_checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"sam_test2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test train_step\n",
    "train_nn = Train(encoder,decoder1,optimizer,tokenizer)\n",
    "enc_hidden = encoder.initialize_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_input_batch\n",
    "# example_target_batch\n",
    "# example_sid_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint\n",
    "checkpoint = tf.train.Checkpoint(optimizer=train_nn.optimizer,\n",
    "                                 encoder=train_nn.encoder,\n",
    "                                 decoder=train_nn.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.5076135, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# speaker model\n",
    "loss = train_nn.train_step(example_input_batch,example_target_batch,enc_hidden,example_sid_batch)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_nn = Train(encoder,decoder2,optimizer,tokenizer)\n",
    "enc_hidden = encoder.initialize_hidden_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0233467, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# spekaer-addressee model\n",
    "loss = train_nn.train_step(example_input_batch,example_target_batch,enc_hidden,\n",
    "                           example_sid_batch,example_aid_batch)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 3\n",
      "enc_hidden.shape: (3, 1000)\n",
      "Epoch 1 Batch 5612 Loss 0.2745\n",
      "Epoch 1 Loss 0.0915\n",
      "Time taken for 1 epoch 43.728718996047974 sec\n",
      "\n",
      "batch_size: 3\n",
      "enc_hidden.shape: (3, 1000)\n",
      "Epoch 2 Batch 5612 Loss 0.2510\n",
      "Epoch 2 Loss 0.0837\n",
      "Time taken for 1 epoch 47.54292893409729 sec\n",
      "\n",
      "batch_size: 3\n",
      "enc_hidden.shape: (3, 1000)\n",
      "Epoch 3 Batch 5612 Loss 0.2600\n",
      "Epoch 3 Loss 0.0867\n",
      "Time taken for 1 epoch 44.630903005599976 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# speaker model\n",
    "train_nn = Train(encoder,decoder1,optimizer,tokenizer)\n",
    "train_nn.run_iter(EPOCHS,False,steps_per_epoch,checkpoint,checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 1 Batch 0 Loss 1.4706\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 1 Batch 1 Loss 1.4594\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 1 Batch 2 Loss 1.3419\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 1 Batch 3 Loss 1.2221\n",
      "Epoch 1 Loss 1.8313\n",
      "Time taken for 1 epoch 205.0519299507141 sec\n",
      "\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 2 Batch 0 Loss 1.0640\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 2 Batch 1 Loss 1.0076\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 2 Batch 2 Loss 0.8958\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 2 Batch 3 Loss 0.9274\n",
      "Epoch 2 Loss 1.2983\n",
      "Time taken for 1 epoch 204.30272889137268 sec\n",
      "\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 3 Batch 0 Loss 0.8793\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 3 Batch 1 Loss 0.9056\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 3 Batch 2 Loss 0.9431\n",
      "batch_size: 16\n",
      "enc_hidden.shape: (16, 1000)\n",
      "Epoch 3 Batch 3 Loss 0.8042\n",
      "Epoch 3 Loss 1.1774\n",
      "Time taken for 1 epoch 204.12451696395874 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# speaker-addressee model\n",
    "train_nn = Train(encoder,decoder2,optimizer,tokenizer)\n",
    "train_nn.run_iter(EPOCHS,True,steps_per_epoch,checkpoint,checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(train_nn,inp,targ,speaker_id,addressee_id=None,batch_size=BATCH_SIZE):\n",
    "    loss = 0\n",
    "    inp = np.asarray(inp)\n",
    "    targ = np.asarray(targ)\n",
    "    speaker_id = np.asarray(speaker_id)\n",
    "    if addressee_id is not None:\n",
    "        addressee_id = np.asarray(addressee_id)\n",
    "#     print(\"test targ[:,1].shape: {}\".format(targ[:,1].shape))\n",
    "    val_size = targ.shape[0]\n",
    "    print(\"val_size: {}\".format(val_size))\n",
    "    num_each_batch = (int)(np.floor(val_size/batch_size))\n",
    "    remaining_num = val_size - num_each_batch * batch_size\n",
    "    if remaining_num == 0:\n",
    "        remaining_num = batch_size\n",
    "    for k in range(0,val_size,batch_size):\n",
    "        start = time.time()\n",
    "        batch_loss = 0\n",
    "        \n",
    "        if (k+batch_size)>=val_size:\n",
    "            print(\"k now +batch_size>=val_size: {}\".format(k))\n",
    "            inputs = inp[k:]\n",
    "            targs = targ[k:]\n",
    "            s_id = speaker_id[k:]\n",
    "#             print(\"s_id.shape: {}\".format(s_id.shape))\n",
    "            if addressee_id is not None:\n",
    "                a_id = addressee_id[k:]\n",
    "            enc_hidden = [tf.zeros((remaining_num, HIDDEN_SIZE)),tf.zeros((remaining_num, HIDDEN_SIZE))]\n",
    "            \n",
    "            enc_out, enc_hidden,enc_c = train_nn.encoder(inputs, enc_hidden)\n",
    "            dec_init_state = [enc_hidden,enc_c]\n",
    "            dec_input = tf.expand_dims([train_nn.tokenizer.word_index['<sos>']]*remaining_num, 1)\n",
    "        else:\n",
    "            print(\"k now: {}\".format(k))\n",
    "            inputs = inp[k:k+batch_size]\n",
    "            targs = targ[k:k+batch_size]\n",
    "            s_id = speaker_id[k:k+batch_size]\n",
    "            print(\"s_id.shape: {}\".format(s_id.shape))\n",
    "            if addressee_id is not None:\n",
    "                a_id = addressee_id[k:k+batch_size]\n",
    "            \n",
    "            enc_hidden = [tf.zeros((batch_size, HIDDEN_SIZE)),tf.zeros((batch_size, HIDDEN_SIZE))]\n",
    "            enc_out, enc_hidden,enc_c = train_nn.encoder(inputs, enc_hidden)\n",
    "            dec_init_state = [enc_hidden,enc_c]\n",
    "            dec_input = tf.expand_dims([train_nn.tokenizer.word_index['<sos>']]*batch_size, 1)\n",
    "            \n",
    "        word_per_line = count_real_word(targs)\n",
    "        \n",
    "        for t in range(targ.shape[1]):\n",
    "            if addressee_id is not None:\n",
    "                predictions, dec_hidden, dec_c,_ = train_nn.decoder(dec_input,enc_out, dec_init_state,s_id,a_id)\n",
    "            else:\n",
    "                predictions, dec_hidden, dec_c,_ = train_nn.decoder(dec_input,enc_out, dec_init_state,s_id)\n",
    "            \n",
    "            # use the max prob one in each sentence in the batch\n",
    "            predicted_id = tf.argmax(predictions,axis=1)\n",
    "            dec_init_state = [dec_hidden,dec_c]\n",
    "#             batch_loss += train_nn.loss_function(targs[:,t], predictions)\n",
    "            loss_ = train_nn.loss_function(targs[:, t], predictions)\n",
    "            batch_loss  += tf.math.reduce_sum(loss_/word_per_line)\n",
    "#             print(\"test predictions[0]: {}\".format(predictions[0]))\n",
    "#             predictions = np.asarray(predictions)\n",
    "#             predicted = tf.constant(predicted_id)\n",
    "            dec_input = tf.expand_dims(predicted_id, 1)\n",
    "        \n",
    "        if (k+batch_size)>=val_size:\n",
    "            loss += batch_loss/batch_size\n",
    "        else:\n",
    "            loss += batch_loss/remaining_num\n",
    "        \n",
    "        print('batch {} Loss {:.4f}'.format(k/batch_size + 1,\n",
    "                                              loss))\n",
    "        print('Time taken {} sec\\n'.format(time.time() - start))  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 16\n",
    "size = 2*batch_sz\n",
    "sample_input_val = dia_val[:size]\n",
    "sample_targ_val = res_val[:size]\n",
    "sample_sid_val = sid_val[:size]\n",
    "sample_aid_val = aid_val[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_size: 32\n",
      "k now: 0\n",
      "s_id.shape: (16,)\n",
      "batch 1.0 Loss 7.2168\n",
      "Time taken 9.33592700958252 sec\n",
      "\n",
      "k now +batch_size>=val_size: 16\n",
      "batch 2.0 Loss 14.7253\n",
      "Time taken 9.266100883483887 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=2420473, shape=(), dtype=float32, numpy=14.725277>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speaker model\n",
    "validation(train_nn,sample_input_val,sample_targ_val,sample_sid_val,batch_size = batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_size: 32\n",
      "k now: 0\n",
      "s_id.shape: (16,)\n",
      "batch 1.0 Loss 7.5812\n",
      "Time taken 37.32085299491882 sec\n",
      "\n",
      "k now +batch_size>=val_size: 16\n",
      "s_id.shape: (16,)\n",
      "batch 2.0 Loss 14.3822\n",
      "Time taken 38.703108072280884 sec\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1738526, shape=(), dtype=float32, numpy=14.382153>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# speaker-addressee model\n",
    "validation(train_nn,sample_input_val,sample_targ_val,sample_sid_val,sample_aid_val,batch_size = batch_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
